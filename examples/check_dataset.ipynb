{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./eztransfer_modelzoo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为环境变量做好准备\n",
    "import os\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_text = \"\"\"\n",
    "HOME=./eztransfer_modelzoo\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv(stream=StringIO(env_text), override=True, verbose=True)\n",
    "os.environ.get(\"HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\code\\\\github\\\\EasyTransfer',\n",
       " 'd:\\\\code\\\\github\\\\EasyTransfer\\\\examples',\n",
       " 'c:\\\\Users\\\\tzh\\\\.vscode\\\\extensions\\\\ms-toolsai.jupyter-2021.11.1001550889\\\\pythonFiles\\\\vscode_datascience_helpers',\n",
       " 'c:\\\\Users\\\\tzh\\\\.vscode\\\\extensions\\\\ms-toolsai.jupyter-2021.11.1001550889\\\\pythonFiles',\n",
       " 'c:\\\\Users\\\\tzh\\\\.vscode\\\\extensions\\\\ms-toolsai.jupyter-2021.11.1001550889\\\\pythonFiles\\\\lib\\\\python',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\python39.zip',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\DLLs',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\lib',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2',\n",
       " '',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Anaconda3\\\\envs\\\\tf2\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 让本地的 easytransfer 的优先级更高\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*********** tf.__version__ is 2.7.0 ******\n",
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "# 导入 easytransfer, 并验证, 因为原始的没有 __version__ 这个参数\n",
    "import importlib\n",
    "import easytransfer as easytransfer\n",
    "importlib.reload(easytransfer)\n",
    "print(easytransfer.__version__)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ipykernel_launcher', '--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"28869bbf-4e65-4636-a638-fbf3b32dc081\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=C:\\\\Users\\\\tzh\\\\AppData\\\\Local\\\\Temp\\\\tmp-8760buZ5JuhzxLln.json']\n"
     ]
    }
   ],
   "source": [
    "# jupyter 特殊操作\n",
    "old_argv = sys.argv\n",
    "print(old_argv)\n",
    "sys.argv = old_argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easytransfer.datasets import CSVReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num_parallel_batches 1\n",
      "INFO:tensorflow:shuffle_buffer_size None\n",
      "INFO:tensorflow:prefetch_buffer_size 1\n",
      "INFO:tensorflow:batch_size 4\n",
      "INFO:tensorflow:distribution_strategy None\n",
      "INFO:tensorflow:num_micro_batches 1\n",
      "INFO:tensorflow:input_schema label:str:1,content:str:1\n",
      "INFO:tensorflow:D:/code/py_nlp_classify/data/small/train.csv, total number of training examples 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<easytransfer.datasets.csv_reader.CSVReader at 0x2280579a7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = CSVReader(\"D:/code/py_nlp_classify/data/small/train.csv\", is_training=True, input_schema=\"label:str:1,content:str:1\", batch_size=4)\n",
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Random shuffle on the whole 1000 training examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {label: (None,), content: (None,)}, types: {label: tf.string, content: tf.string}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = reader.get_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'game', b'house', b'car', b'world'], dtype=object)>,\n",
       " 'content': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       " array([b'\\xe5\\x85\\xa8\\xe6\\xb0\\x91k\\xe6\\xad\\x8c\\xe5\\xbd\\x95\\xe9\\x9f\\xb3\\xe6\\x97\\xb6\\xe4\\xbc\\x9a\\xe5\\x87\\xba\\xe7\\x8e\\xb0\\xe5\\x8e\\x9f\\xe5\\xa3\\xb0\\xe5\\xba\\x94\\xe8\\xaf\\xa5\\xe6\\x80\\x8e\\xe4\\xb9\\x88\\xe5\\x8a\\x9e\\xef\\xbc\\x9f',\n",
       "        b'\\xe5\\x86\\x9c\\xe6\\x9d\\x91\\xe6\\x89\\xbf\\xe5\\x8c\\x85\\xe5\\x9c\\x9f\\xe5\\x9c\\xb0100\\xe4\\xba\\xa9\\xef\\xbc\\x8c\\xe5\\x9b\\xbd\\xe5\\xae\\xb6\\xe6\\x9c\\x89\\xe4\\xbb\\x80\\xe4\\xb9\\x88\\xe8\\xa1\\xa5\\xe8\\xb4\\xb4\\xef\\xbc\\x9f',\n",
       "        b'\\xe5\\xbe\\xb7\\xe7\\xb3\\xbb\\xe5\\xa4\\xa7\\xe6\\x88\\x98\\xe7\\xbe\\x8e\\xe7\\xb3\\xbb\\xef\\xbc\\x8c\\xe6\\x96\\xaf\\xe6\\x9f\\xaf\\xe8\\xbe\\xbe\\xe6\\x9f\\xaf\\xe7\\xb1\\xb3\\xe5\\x85\\x8b\\xe5\\xaf\\xb9\\xe6\\xa0\\x87\\xe5\\x88\\xab\\xe5\\x85\\x8b\\xe6\\x98\\x82\\xe7\\xa7\\x91\\xe6\\x8b\\x89\\xef\\xbc\\x8c\\xe8\\xb0\\x81\\xe6\\x9b\\xb4\\xe5\\x8f\\x97\\xe5\\xb9\\xb4\\xe8\\xbd\\xbb\\xe4\\xba\\xba\\xe6\\xac\\xa2\\xe8\\xbf\\x8e SUV,\\xe6\\x9f\\xaf\\xe7\\xb1\\xb3\\xe5\\x85\\x8b,\\xe6\\x98\\x82\\xe7\\xa7\\x91\\xe6\\x8b\\x89,\\xe6\\x96\\xaf\\xe6\\x9f\\xaf\\xe8\\xbe\\xbe,GS7,GS8,\\xe8\\xbd\\xa6\\xe8\\xba\\xab\\xe5\\xb0\\xba\\xe5\\xaf\\xb8',\n",
       "        b'\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe4\\xb8\\xba\\xe4\\xbb\\x80\\xe4\\xb9\\x88\\xe4\\xb8\\x8d\\xe6\\x84\\xbf\\xe6\\x84\\x8f\\xe4\\xb8\\x8b\\xe5\\x8e\\xbb\\xef\\xbc\\x8c\\xe4\\xbc\\x9a\\xe9\\x81\\xad\\xe6\\xb8\\x85\\xe7\\xae\\x97\\xe5\\x90\\x97\\xef\\xbc\\x9f'],\n",
       "       dtype=object)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'content'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (1, 512)\n",
      "input_shape [1, 512]\n",
      "从文件中还原预训练模型\n",
      "INFO:tensorflow:Load weights from D:/code/py_nlp_classify/model/bert/google-bert-base-zh/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "from easytransfer.model_zoo.modeling_bert import BertConfig, BertPreTrainedModel, MyBertPreTrainedModel\n",
    "bert_config_path = \"D:/code/py_nlp_classify/model/bert/google-bert-base-zh/config.json\"\n",
    "bert_config = BertConfig.get(bert_config_path)\n",
    "my_bert_model = MyBertPreTrainedModel(bert_config)\n",
    "# 原来是要调用过后才生成 weights\n",
    "out_put = my_bert_model(my_bert_model.dummy_inputs(512), mode=\"eval\", output_features=False)\n",
    "archive_file = \"D:/code/py_nlp_classify/model/bert/google-bert-base-zh/bert_model.ckpt\"\n",
    "my_bert_model._init_from_pretrained_model(archive_file)\n",
    "\n",
    "# my_bert_model(my_bert_model.dummy_inputs(512), mode=\"eval\", output_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***************** modelZooBasePath ./eztransfer_modelzoo\\.eztransfer_modelzoo ***************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from easytransfer import preprocessors, Config\n",
    "config_json = json.load(open(\"user_config_tf2_test.json\", \"r\", encoding=\"utf-8\"))\n",
    "config = Config(mode=\"train_and_evaluate_on_the_fly\", config_json=config_json)\n",
    "preprocessor = preprocessors.get_preprocessor(\n",
    "    \"D:/code/py_nlp_classify/model/bert/google-bert-base-zh/bert_model.ckpt\", user_defined_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(4, 64), dtype=int64, numpy=\n",
       "  array([[ 101, 1963, 3362,  872, 3221,  671, 1399, 5439, 2360, 8024,  872,\n",
       "          2190, 4408,  677, 4638, 2345, 4495, 3300,  784,  720, 6206, 3724,\n",
       "          8043,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101,  671, 3340, 2544,  928, 1440, 6401,  872, 1762, 2813, 2336,\n",
       "          2582,  720, 1391, 1600, 4381,  727, 6579, 8013, 4125, 6862, 3119,\n",
       "          5966, 8013,  691, 1068, 6125,  117, 1920, 4215, 2397,  692,  117,\n",
       "          2476, 1294, 1409,  117, 4607, 6205, 3959,  117,  702, 1736,  102,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 3125,  752, 8038, 3330, 5439, 3727, 3612, 7178,  679, 6820,\n",
       "          8024, 1155,  671, 2797, 2341,  886, 1975, 6369, 8024, 6206, 1726,\n",
       "          6609, 6572,  511, 1927, 7789, 5862, 7790,  117, 3330, 5439, 3727,\n",
       "           117, 6432, 6887,  117, 3341, 1168, 1155,  671, 2797, 2157,  704,\n",
       "           117, 1155,  671, 2797,  102,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 3342, 1045, 3633, 5314, 6756, 1217, 3779, 1450, 8024, 3340,\n",
       "          2094, 4294, 5314, 1213, 8024, 1146, 1146, 7164, 5314, 3342, 1045,\n",
       "          3341,  749,  671, 3418, 4170, 8013, 3342, 1045,  117, 1921, 3823,\n",
       "           117, 3342, 1045, 4638, 2571,  727, 4495, 3833,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "        dtype=int64)>,\n",
       "  <tf.Tensor: shape=(4, 64), dtype=int64, numpy=\n",
       "  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "        dtype=int64)>,\n",
       "  <tf.Tensor: shape=(4, 64), dtype=int64, numpy=\n",
       "  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "        dtype=int64)>],\n",
       " <tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       " array([[ 8],\n",
       "        [10],\n",
       "        [13],\n",
       "        [ 2]], dtype=int64)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_out = preprocessor(next(iter(dataset)))\n",
    "process_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "3\n",
      "tf.Tensor(\n",
      "[[ 8]\n",
      " [10]\n",
      " [13]\n",
      " [ 2]], shape=(4, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(type(process_out))\n",
    "print(len(process_out))\n",
    "\n",
    "print(len(process_out[0]))\n",
    "print(process_out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling string_split (from tensorflow.python.ops.ragged.ragged_string_ops) with delimiter is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "delimiter is deprecated, please use sep instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 4, 64), dtype=int64, numpy=\n",
       " array([[[  101,  4692,  5401,  1196,  2110,  5739,  6427,  8271,   758,\n",
       "           3299,  6841,  1196,  2900,  1298,  6963,  2356,  1957,  2111,\n",
       "            117,  1318,  4886,   117,  8450, 10712, 10936,   117,  4868,\n",
       "           2968,  1909,  3821,  1046,   117,  5294,  5276,  1957,  2094,\n",
       "           1745,  7063,   117,  3654,  6854,  1398,  2495,   117, 10373,\n",
       "            117,  3617,  3307,  6963,  2356,   117,  3449,  2209,  5384,\n",
       "           3172,   117,  7946,  7178,  5526,  1765,   117,  1628,  6117,\n",
       "            102],\n",
       "         [  101,  2798,  6371,  6399,   697,  1453,  4638,  4685,   779,\n",
       "           4511,  4960,  4197,   779,   749,   677,  3341,  8024,  2867,\n",
       "           5318,  1400,  8024,   800,  4994,  4197,  2828,  2769,  2861,\n",
       "           7946,   749,  8043,  8013,  4685,   779,   117,  1062,  1218,\n",
       "           1447,   117,  4685,   779,  4511,   117,  4510,  2512,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  2769,   812,  5558,   677,  4638,  6817,  1220,  7490,\n",
       "           4955,  4994,  3341,  5632,  1525,  7027,  8043,  3221,  1415,\n",
       "           4696,  4638,  3221,  4289,  3300,  2792,   966,  8013,  3696,\n",
       "           4965,   117,  5798,  4506,   117, 11169,   117, 10138,  8418,\n",
       "           8374,  8175,   117,  2135,  4965,   117, 12302,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  6821,  7027,   679,   788,  3300,  6408,  1469,  6823,\n",
       "           3175,  8024,  6820,  3300,   855,  2207,  1995,  1995,  4448,\n",
       "           7599, 10647,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       " array([[ 2],\n",
       "        [13],\n",
       "        [ 1],\n",
       "        [ 4]], dtype=int64)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = reader.get_dataset(preprocessor)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_4:0' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "from easytransfer import layers\n",
    "import tensorflow.keras as keras\n",
    "inputs = tf.keras.Input(shape=(None, None,), dtype=tf.int64)\n",
    "\n",
    "dense = layers.Dense(config.num_labels, kernel_initializer=layers.get_initializer(0.02), name=\"dense\")\n",
    "_, pooled_output = my_bert_model(inputs)\n",
    "output = dense(pooled_output)\n",
    "my_model = keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None)]      0         \n",
      "                                                                 \n",
      " my_bert_pre_trained_model (  ((None, None, 768),      102882442 \n",
      " MyBertPreTrainedModel)       (None, 768))                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                11535     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,893,977\n",
      "Trainable params: 102,893,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easytransfer.optimizers.adam_weight_decay_optimizer import AdamWeightDecayOptimizer\n",
    "\n",
    "my_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # optimizer=AdamWeightDecayOptimizer(\n",
    "    #     learning_rate=1e-5,\n",
    "    #     weight_decay_rate=0,\n",
    "    #     beta_1=0.9,\n",
    "    #     beta_2=0.999,\n",
    "    #     epsilon=1e-6,\n",
    "    #     # 有几个层不使用权重衰减系数\n",
    "    #     exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n",
    "    # ),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=1e-5,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.99,\n",
    "        epsilon=1e-6,\n",
    "    ),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'> (4, 64)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> (4, 64)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> (4, 64)\n"
     ]
    }
   ],
   "source": [
    "for x in next(iter(dataset))[0]:\n",
    "    print(type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num_parallel_batches 1\n",
      "INFO:tensorflow:shuffle_buffer_size None\n",
      "INFO:tensorflow:prefetch_buffer_size 1\n",
      "INFO:tensorflow:batch_size 4\n",
      "INFO:tensorflow:distribution_strategy None\n",
      "INFO:tensorflow:num_micro_batches 1\n",
      "INFO:tensorflow:input_schema label:str:1,content:str:1\n",
      "INFO:tensorflow:D:/code/py_nlp_classify/data/small/dev.csv, total number of training examples 100\n",
      "INFO:tensorflow:Random shuffle on the whole 100 training examples\n",
      "Epoch 1/5\n",
      "input_shape (3, None, 64)\n",
      "input_shape [<tf.Tensor 'model/my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, 64]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_bert_pre_trained_model/cls/predictions/output_bias:0', 'my_bert_pre_trained_model/cls/predictions/transform/dense/kernel:0', 'my_bert_pre_trained_model/cls/predictions/transform/dense/bias:0', 'my_bert_pre_trained_model/cls/predictions/transform/LayerNorm/gamma:0', 'my_bert_pre_trained_model/cls/predictions/transform/LayerNorm/beta:0', 'my_bert_pre_trained_model/cls/seq_relationship/output_weights:0', 'my_bert_pre_trained_model/cls/seq_relationship/output_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "input_shape (3, None, 64)\n",
      "input_shape [<tf.Tensor 'model/my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, 64]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_bert_pre_trained_model/cls/predictions/output_bias:0', 'my_bert_pre_trained_model/cls/predictions/transform/dense/kernel:0', 'my_bert_pre_trained_model/cls/predictions/transform/dense/bias:0', 'my_bert_pre_trained_model/cls/predictions/transform/LayerNorm/gamma:0', 'my_bert_pre_trained_model/cls/predictions/transform/LayerNorm/beta:0', 'my_bert_pre_trained_model/cls/seq_relationship/output_weights:0', 'my_bert_pre_trained_model/cls/seq_relationship/output_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "    250/Unknown - 38s 109ms/step - loss: 2.2736 - sparse_categorical_accuracy: 0.3240input_shape (3, None, 64)\n",
      "input_shape [<tf.Tensor 'model/my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, 64]\n",
      "250/250 [==============================] - 41s 123ms/step - loss: 2.2736 - sparse_categorical_accuracy: 0.3240 - val_loss: 1.8540 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 1.4634 - sparse_categorical_accuracy: 0.6260 - val_loss: 1.5011 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 1.0082 - sparse_categorical_accuracy: 0.7320 - val_loss: 1.3898 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.6886 - sparse_categorical_accuracy: 0.8390 - val_loss: 1.4362 - val_sparse_categorical_accuracy: 0.5500\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.4646 - sparse_categorical_accuracy: 0.8940 - val_loss: 1.4210 - val_sparse_categorical_accuracy: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a9fb3328e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reader = CSVReader(\"D:/code/py_nlp_classify/data/small/dev.csv\", is_training=True, input_schema=\"label:str:1,content:str:1\", batch_size=4)\n",
    "eval_dataset = eval_reader.get_dataset(preprocessor)\n",
    "my_model.fit(x=dataset, epochs=5, validation_data=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 43ms/step - loss: 1.4210 - sparse_categorical_accuracy: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.421048879623413, 0.5899999737739563]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'model/my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'model/my_bert_pre_trained_model/bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'my_bert_pre_trained_model/bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'bert/strided_slice_4:0' shape=() dtype=int32>]\n",
      "input_shape (None, None, None)\n",
      "input_shape [<tf.Tensor 'bert/strided_slice_3:0' shape=() dtype=int32>, <tf.Tensor 'bert/strided_slice_4:0' shape=() dtype=int32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bert_layer_call_fn, bert_layer_call_and_return_conditional_losses, cls/predictions_layer_call_fn, cls/predictions_layer_call_and_return_conditional_losses, cls/seq_relationship_layer_call_fn while saving (showing 5 of 830). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A985ACA4C0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A985AAFC10> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A985AA5280> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A9878DC0A0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A987916850> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A985AF8DF0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A98791C6A0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A9879149A0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A9878D35E0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A987849880> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A9878F8910> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<easytransfer.layers.attention.Attention object at 0x000002A98786EB50> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'easytransfer.layers.attention.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "my_model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None)]      0         \n",
      "                                                                 \n",
      " my_bert_pre_trained_model (  ((None, None, 768),      102882442 \n",
      " MyBertPreTrainedModel)       (None, 768))                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                11535     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,893,977\n",
      "Trainable params: 102,893,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 42ms/step - loss: 1.4210 - sparse_categorical_accuracy: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.421048879623413, 0.5899999737739563]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c369c7a0bd18095d69cc6bcfdfaf93c8e305f9651a20b05d28ea042855c27d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
